boxplot(LB ~ NSP, data = mydata,
main = "Box Plot of LB by NSP",
xlab = "NSP",
ylab = "LB",
col = c("lightblue", "lightgreen", "pink"))
# Plot variable importance
plot(varImp(boo), main = "Variable Importance Plot")
# Make predictions on test set
p <- predict(boo, test, type = 'raw')
# Confusion matrix
confusionMatrix(p, test$NSP)
# Plot confusion matrix
library(ggplot2)
cm <- confusionMatrix(p, test$NSP)
cm_df <- as.data.frame(cm$table)
ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), vjust = 0.5, color = "white") +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Confusion Matrix",
x = "Actual",
y = "Predicted")
# Load libraries
library(caret)
library(e1071)
library(xgboost)
library(ggplot2)
# Load the data
mydata <- read.csv('https://raw.githubusercontent.com/bkrai/Statistical-Modeling-and-Graphs-with-R/main/Cardiotocographic.csv', header = T)
# Convert NSP to a factor
mydata$NSP <- as.factor(mydata$NSP)
# Partition the data
set.seed(123)
ind <- createDataPartition(mydata$NSP, p = 0.8, list = FALSE)
train <- mydata[ind,]
test <- mydata[-ind,]
# Build XGBoost Model
set.seed(123)
cvcontrol <- trainControl(method = "repeatedcv", number = 5, repeats = 1, allowParallel = TRUE)
xgb_grid <- expand.grid(nrounds = 500,
max_depth = 4,
eta = 0.28,
gamma = 1.8,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1)
xgb_model <- train(NSP ~ ., data = train, method = "xgbTree", trControl = cvcontrol, tuneGrid = xgb_grid)
# Variable Importance Plot
xgb_varimp <- varImp(xgb_model)
plot(xgb_varimp)
# Model Evaluation
xgb_pred <- predict(xgb_model, test, type = "raw")
conf_matrix <- confusionMatrix(xgb_pred, test$NSP)
# Confusion Matrix Plot
confusionMatrixPlot <- ggplot(data = as.data.frame(conf_matrix$table),
aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white") +
labs(title = "Confusion Matrix", x = "True Class", y = "Predicted Class") +
theme_minimal()
print(confusionMatrixPlot)
# Plot, RMSE, R-square
ba <- predict(bag,  test)
set.seed(1234)
bag <- train(medv ~ .,
data = train,
method = "treebag",
trControl = cvcontrol,
importance = TRUE)
print(class(bag))
# Check for successful training
if (exists("bag")) {
print("Model trained successfully")
print(summary(bag))
} else {
print("Model training failed")
}
# Predict using 'bag' model
predictions <- predict(bag, test)
print(names(train))
# Ensure all libraries are loaded
library(caret)
# Assuming you've loaded your data correctly
if ("medv" %in% names(train)) {
set.seed(1234)
bag <- train(medv ~ ., data = train, method = "treebag",
trControl = cvcontrol, importance = TRUE)
if (inherits(bag, "train")) {
print("Model trained successfully")
print(summary(bag))
} else {
print("Model training might have failed, check the output for errors.")
}
} else {
print("The variable 'medv' does not exist in the training dataset.")
}
# Assuming you've loaded and prepared your data correctly, and the correct libraries are loaded
library(caret)
library(mlbench)  # If needed for other parts of your code
install.packages("mlbench")
# Assuming you've loaded and prepared your data correctly, and the correct libraries are loaded
library(caret)
library(mlbench)  # If needed for other parts of your code
set.seed(1234)
# Update trainControl for classification (if needed)
cvcontrol <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
classProbs = TRUE,  # Probability prediction for classification
summaryFunction = twoClassSummary)
# Train a classification model using tree bagging
bag <- train(NSP ~ ., data = train, method = "treebag",
trControl = cvcontrol, importance = TRUE)
print(levels(train$NSP))
levels(train$NSP) <- make.names(levels(train$NSP))
print(levels(train$NSP))
set.seed(1234)
bag <- train(NSP ~ ., data = train, method = "treebag",
trControl = cvcontrol, importance = TRUE)
# Update trainControl for multi-class classification
cvcontrol <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
classProbs = TRUE,  # Probability prediction for classification
summaryFunction = multiClassSummary)  # Use appropriate summary function for multi-class
# Retry training the model
set.seed(1234)
bag <- train(NSP ~ ., data = train, method = "treebag",
trControl = cvcontrol, importance = TRUE)
installed.packages("MLmetrics")
# Update trainControl for multi-class classification
cvcontrol <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
classProbs = TRUE,  # Probability prediction for classification
summaryFunction = multiClassSummary)  # Use appropriate summary function for multi-class
# Retry training the model
set.seed(1234)
bag <- train(NSP ~ ., data = train, method = "treebag",
trControl = cvcontrol, importance = TRUE)
library(MLmetrics)
install.packages("MLmetrics")
# Load the MLmetrics package
library(MLmetrics)
# Retry training the model
set.seed(1234)
bag <- train(NSP ~ ., data = train, method = "treebag",
trControl = cvcontrol, importance = TRUE)
# Check the output
if (exists("bag")) {
print("Model trained successfully")
print(summary(bag))
} else {
print("Model training might have failed, check the output for errors.")
}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(xgboost)
library(readr)
library(lime)
install.packages("lime")
library(caret)
library(xgboost)
library(readr)
library(lime)
# Load data
mydata <- read_csv("/Users/jeevankumarbanoth/Downloads/cardiotocographic.csv")
# Convert NSP to factor
mydata$NSP <- as.factor(mydata$NSP)
# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(mydata$NSP, p = 0.8, list = FALSE)
train <- mydata[trainIndex, ]
test <- mydata[-trainIndex, ]
# Set up cross-validation
cvcontrol <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1,
allowParallel = TRUE)
# Train XGBoost model
set.seed(123)
boo <- train(NSP ~ .,
data = train,
method = "xgbTree",
trControl = cvcontrol,
tuneGrid = expand.grid(nrounds = 500,
max_depth = 4,
eta = 0.28,
gamma = 1.8,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1))
# Plot variable importance
plot(varImp(boo), main = "Variable Importance Plot")
# Make predictions on test set
p <- predict(boo, test, type = 'raw')
# Confusion matrix
confusionMatrix(p, test$NSP)
# Plot confusion matrix
library(ggplot2)
cm <- confusionMatrix(p, test$NSP)
cm_df <- as.data.frame(cm$table)
ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), vjust = 0.5, color = "white") +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Confusion Matrix",
x = "Actual",
y = "Predicted")
# Explain predictions
explainer <- lime(test[1:3,], boo, n_bins = 5)
explanation <- explain(x = test[1:3,],
explainer = explainer,
n_features = 5)
# Load libraries
library(mlbench)
library(caret)
library(e1071)
library(lime)
library(ggplot2)
# Load cardiotocographic data
mydata <- read.csv('https://raw.githubusercontent.com/bkrai/Statistical-Modeling-and-Graphs-with-R/main/Cardiotocographic.csv', header = T)
# Convert NSP to a factor
mydata$NSP <- as.factor(mydata$NSP)
# Partition the data
set.seed(1234)
ind <- sample(2, nrow(mydata), replace = T, prob = c(0.8, 0.2))
train <- mydata[ind == 1,]
test <- mydata[ind == 2,]
# Bagging
set.seed(1234)
cvcontrol <- trainControl(method="repeatedcv", number = 5, repeats = 2, allowParallel=TRUE)
set.seed(1234)
bag <- train(NSP ~ ., data=train, method="treebag", trControl=cvcontrol, importance=TRUE)
# Variable Importance Plot for Bagging
plot(varImp(bag))
# Predictions, Accuracy, and Kappa for Bagging
bag_pred <- predict(bag, test)
bag_cm <- confusionMatrix(bag_pred, test$NSP)
bag_cm$overall["Accuracy"]
bag_cm$overall["Kappa"]
# Random Forest
set.seed(1234)
forest <- train(NSP ~ ., data=train, method="rf", trControl=cvcontrol, importance=TRUE)
data <- read.csv('/Users/jeevankumarbanoth/Downloads/data.csv')
str(data)
data$NSP <- as.factor(data$NSP)
data$X1 <- as.factor(data$X1)
str(data)
table(data$NSP)
set.seed(123)
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data[ind==1,]
test <- data[ind==2,]
library(caret)
set.seed(123)  # for reproducibility
# Define the control using cross-validation
fitControl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,  # if classification
summaryFunction = defaultSummary  # or another appropriate summary function
)
# Train the model
model <- train(X1 ~ ., data = data, method = "rf", trControl = fitControl)  # using random forest as an example
# Random Forest model
library(randomForest)
set.seed(222)
rf <- randomForest(NSP~., data=train,
ntree = 300,
mtry = 8,
importance = TRUE,
proximity = TRUE)
data <- read.csv('/Users/jeevankumarbanoth/Downloads/data.csv')str(data)
data <- read.csv('/Users/jeevankumarbanoth/Downloads/data.csv')
str(data)
data$X1 <- as.factor(data$X1)
str(data)
# Data Partition
set.seed(123)
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data[ind==1,]
test <- data[ind==2,]
# Random Forest model
library(randomForest)
set.seed(222)
rf <- randomForest(NSP~., data=train,
ntree = 300,
mtry = 8,
importance = TRUE,
proximity = TRUE)
print(colnames(data))
data$NSP <- as.factor(data$SomeOtherColumn)  # Replace 'SomeOtherColumn' with the actual column name
print(head(train))
print(head(test))
library(randomForest)
set.seed(222)
rf <- randomForest(NSP ~ ., data = train,
ntree = 300,
mtry = 8,
importance = TRUE,
proximity = TRUE)
data$TargetVariable <- as.factor(data$X1)  # Replace X1 with the actual column name intended as the target
set.seed(123)  # Ensuring reproducibility
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data[ind==1,]
test <- data[ind==2,]
library(randomForest)
set.seed(222)
rf <- randomForest(TargetVariable ~ ., data = train,
ntree = 300,
mtry = 8,
importance = TRUE,
proximity = TRUE)
sapply(train, function(x) if(is.factor(x)) length(levels(x)) else NA)
# Group levels that make up less than 1% of the total
freq <- table(train$X1) / nrow(train)
levels_to_group <- names(freq[freq < 0.01])  # Adjust threshold as needed
train$X1[train$X1 %in% levels_to_group] <- 'Other'
train$X1 <- factor(train$X1)
train$X1 <- NULL  # Remove the variable completely
train$X1 <- as.numeric(as.character(train$X1))
set.seed(222)
rf <- randomForest(TargetVariable ~ ., data = train,
ntree = 300,
mtry = 8,
importance = TRUE,
proximity = TRUE)
data <- read.csv('/Users/jeevankumarbanoth/Downloads/data.csv')
data$X0 <- as.factor(data$X0)
set.seed(123)
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data[ind==1,]
test <- data[ind==2,]
library(randomForest)
library(caret)
set.seed(222)
rf <- randomForest(X0 ~ ., data=train, ntree=300, mtry=8, importance=TRUE, proximity=TRUE)
# Reduce the dataset size for training (sample 50% of the data)
set.seed(123)
train_sample <- train[sample(nrow(train), size = nrow(train) * 0.5), ]
# Random Forest model with reduced parameters
set.seed(222)
rf <- randomForest(X0 ~ ., data=train_sample, ntree=100, mtry=4, importance=TRUE)
# Check performance
p1 <- predict(rf, train_sample)
cm_train <- confusionMatrix(p1, train_sample$X0)
print(cm_train)
p2 <- predict(rf, test)
cm_test <- confusionMatrix(p2, test$X0)
print(cm_test)
# Plot error rate
plot(rf)
t <- tuneRF(train[,-ncol(train)], train[,ncol(train)], stepFactor=0.5, plot=TRUE, ntreeTry=300, trace=TRUE, improve=0.05)
hist(treesize(rf), main="No. of Nodes for the Trees", col="green")
varImpPlot(rf, sort=T, n.var=10, main="Top 10 - Variable Importance")
importance(rf)
# Calculate importance and convert to a data frame
imp <- importance(rf)
imp_df <- data.frame(Variable = rownames(imp), Importance = imp[, '%IncMSE'])
# Prepare the data frame with variable names and their importance scores
imp_df <- data.frame(Variable = rownames(importance(rf)), Importance = importance(rf)[, "MeanDecreaseAccuracy"])
# Sort by importance
imp_df <- imp_df[order(-imp_df$Importance), ]
# Select the top 10 variables for plotting
top10_imp <- head(imp_df, 10)
# Use a bar plot to display the importance of the top 10 variables
barplot(top10_imp$Importance, names.arg = top10_imp$Variable, las = 2,
main = "Top 10 - Variable Importance", col = "blue", horiz = TRUE, cex.names = 0.7)
# If space is tight for variable names, you might consider increasing the plotting window or adjusting `cex.names`
hist(treesize(rf), main="No. of Nodes for the Trees", col="green")
varImpPlot(rf, sort=T, n.var=10, main="Top 10 - Variable Importance")
getTree(rf, 1, labelVar=TRUE)
# Multi-dimensional Scaling Plot of Proximity Matrix
MDSplot(rf, train$X0)
# Assuming 'rf' is your random forest model
importance_data <- importance(rf)  # Get importance data
# Create a data frame of the variables and their MeanDecreaseAccuracy
var_importance <- data.frame(
Variable = rownames(importance_data),
Importance = importance_data[, "MeanDecreaseAccuracy"]
)
# Remove any NA values if necessary (just in case)
var_importance <- na.omit(var_importance)
# Sort by importance
var_importance <- var_importance[order(-var_importance$Importance), ]
# Select the top 10
top10_importance <- head(var_importance, 10)
# Plotting
barplot(top10_importance$Importance, names.arg = top10_importance$Variable,
main = "Top 10 Variable Importance", horiz = TRUE,
las = 1, col = "blue", cex.names = 0.7)
# Check for any Inf or NaN values in your data
sum(is.infinite(var_importance$Importance))  # Should be 0
sum(is.nan(var_importance$Importance))        # Should be 0
# Example of rerunning with proximity matrix and reduced settings
set.seed(222)
rf <- randomForest(X0 ~ ., data = train, ntree = 100,  # reduced number of trees
mtry = 8, importance = TRUE, proximity = TRUE)
if (!is.null(rf$proximity)) {
# Compute MDS coordinates from the proximity matrix
mds_coords <- cmdscale(as.dist(1 - rf$proximity), k = 2)  # Using classical MDS
# Plotting using base R
plot(mds_coords[,1], mds_coords[,2], col = as.factor(train$X0), pch = 19,
xlab = "MDS1", ylab = "MDS2", main = "MDS Plot of Proximity Matrix")
legend("topright", legend = levels(as.factor(train$X0)), col = 1:length(levels(as.factor(train$X0))), p
if (!is.null(rf$proximity)) {
if (!is.null(rf$proximity)) {
# Compute MDS coordinates from the proximity matrix
mds_coords <- cmdscale(as.dist(1 - rf$proximity), k = 2)  # Using classical MDS
# Plotting using base R
plot(mds_coords[,1], mds_coords[,2], col = as.factor(train$X0), pch = 19,
xlab = "MDS1", ylab = "MDS2", main = "MDS Plot of Proximity Matrix")
legend("topright", legend = levels(as.factor(train$X0)), col = 1:length(levels(as.factor(train$X0))), pch = 19)
}
ncol(train) - 1  # Minus 1 because we exclude the target variable
# Load the data
data <- read.csv('/Users/jeevankumarbanoth/Downloads/data.csv')
# Convert the supposed categorical variables to factors
data$X1 <- as.factor(data$X1)
data$X0 <- as.factor(data$X0)  # Assuming X0 is the target variable for this example
# Check structure
str(data)
# Set seed for reproducibility
set.seed(123)
# Splitting data into training and testing
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data[ind == 1,]
test <- data[ind == 2,]
# Load the randomForest library
library(randomForest)
# Set seed for model reproducibility
set.seed(222)
# Train the model
rf <- randomForest(X0 ~ . - X11.7.17.9.30 - X,  # Exclude non-predictive or problematic features
data = train,
ntree = 300,
mtry = 8,
importance = TRUE,
proximity = TRUE)
# Calculate frequency of each level in X1
freq <- table(train$X1) / nrow(train)
# Levels that make up less than 1% of the data
levels_to_group <- names(freq[freq < 0.01])
# Grouping infrequent levels into 'Other'
train$X1 <- as.factor(ifelse(train$X1 %in% levels_to_group, 'Other', as.character(train$X1)))
test$X1 <- as.factor(ifelse(test$X1 %in% levels_to_group, 'Other', as.character(test$X1)))
# Checking how many levels are now in X1
length(levels(train$X1))
# Install and load ranger if necessary
if (!require("ranger")) install.packages("ranger")
library(ranger)
# Using ranger for random forest modeling
rf_ranger <- ranger(X0 ~ . - X11.7.17.9.30 - X, data = train,
num.trees = 300,
mtry = 8,
importance = 'impurity',
save.memory = TRUE)
# Counting variables
n_vars <- ncol(train) - length(c("X11.7.17.9.30", "X")) # adjust the exclusion list as needed
print(n_vars)
# Adjust mtry based on the number of variables
mtry_value <- min(8, n_vars) # Ensure mtry is not larger than the number of available variables
# Run ranger with the adjusted mtry value
rf_ranger <- ranger(X0 ~ . - X11.7.17.9.30 - X, data = train,
num.trees = 300,
mtry = mtry_value,
importance = 'impurity',
save.memory = TRUE)
# Read the data
data <- read.csv('/Users/jeevankumarbanoth/Downloads/data.csv')
# Explore the data
str(data)
summary(data)
# Convert the categorical variables to factors
data$X1 <- as.factor(data$X1)
data$X3 <- as.factor(data$X3)
data <- read.csv('/Users/jeevankumarbanoth/Downloads/data.csv')
# Explore the data
str(data)
# Summary of data
summary(data)
# Convert the categorical variables to factors where applicable
data$X1 <- as.factor(data$X1)
data <- data[, !(names(data) %in% c("X11.7.17.9.30", "X"))]
str(data)
# Convert the categorical variables to factors
data$X1 <- as.factor(data$X1)
data$X3 <- as.factor(data$X3)
names(data)
str(data)
data <- data[, !(names(data) %in% c("X11.7.17.9.30", "X"))]
# Display the structure after modification
str(data)
library(ranger)
# Prepare the data by removing non-predictive or problematic features (if any)
data_clean <- data[, !(names(data) %in% c("X11.7.17.9.30", "X"))]
# Assuming X0 is the response variable and all other are predictors
rf_ranger <- ranger(X0 ~ ., data = data_clean,
num.trees = 300,
mtry = floor(sqrt(ncol(data_clean) - 1)), # since X0 is not a predictor
importance = 'impurity',
save.memory = TRUE)
# Check the model's output
print(rf_ranger)
# Convert the categorical variables to factors
data$X1 <- as.factor(data$X1)
data$X3 <- as.factor(data$X3)
